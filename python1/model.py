from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import sys
import json
import os


# # è®€å–nodejså‚³ééä¾†çš„JSONå­—ç¬¦ä¸²
data = json.loads(sys.argv[1])
text = data['text']

# input_data = sys.stdin.readline()
# data = json.loads(input_data)
# text = data['text']
# åœ¨é€™è£¡é€²è¡Œä½ çš„æ–‡å­—é›²ç”Ÿæˆç¨‹åºï¼Œç”¢ç”Ÿåœ–åƒæª”æ¡ˆ

# å°‡åœ–åƒæª”æ¡ˆä¿å­˜åˆ°æŒ‡å®šè·¯å¾‘
# imagePath = "D:\user\Desktop\MERN\mernchat\python1\img\wordcloud87.png"
# wordCloudImage.save(imagePath)

# è¿”å›åœ–åƒæª”æ¡ˆè·¯å¾‘åˆ°nodejs

# è¼‰å…¥æ¨¡å‹å’Œtokenizer
model_name = "philschmid/bart-large-cnn-samsum"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# è¼¸å…¥åŸæ–‡
# source_text = '''Jeff: Can I train a ğŸ¤— Transformers model on Amazon SageMaker?
#                 Philipp: Sure you can use the new Hugging Face Deep Learning Container.
#                 Jeff: ok.
#                 Jeff: and how can I get started?
#                 Jeff: where can I find documentation?
#                 Philipp: ok, ok you can find everything here.'''

# å°‡åŸæ–‡ç·¨ç¢¼ç‚ºtokenizeræ‰€éœ€æ ¼å¼
input_ids = tokenizer.encode(text, return_tensors="pt")

# ç”Ÿæˆæ‘˜è¦
summary_ids = model.generate(
    input_ids, num_beams=4, max_length=100, early_stopping=True)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# é¡¯ç¤ºæ‘˜è¦
print(summary)

# print(json.dumps({'summary': summary}))

# import sys
# import json


# data = json.loads(sys.argv[1])

# åœ¨é€™è£¡é€²è¡Œä½ çš„æ–‡å­—é›²ç”Ÿæˆç¨‹åºï¼Œç”¢ç”Ÿåœ–åƒæª”æ¡ˆ

# å°‡åœ–åƒæª”æ¡ˆä¿å­˜åˆ°æŒ‡å®šè·¯å¾‘
# imagePath = "D:\user\Desktop\MERN\mernchat\python1\img\wordcloud87.png"
# wordCloudImage.save(imagePath)

# è¿”å›åœ–åƒæª”æ¡ˆè·¯å¾‘åˆ°nodejs
# print(data)
